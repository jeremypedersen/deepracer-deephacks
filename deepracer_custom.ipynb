{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DeepRacer models using SageMaker and RoboMaker\n",
    "---\n",
    "\n",
    "This notebook borrows heavily from AWS's [DeepRacer 400L workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/66473261-de66-42a1-b280-3e0ec87aee26/en-US).\n",
    "\n",
    "\n",
    "### How can I use this notebook? \n",
    "\n",
    "Training new DeepRacer models directly with SageMaker and RoboMaker offers the flexibility to: \n",
    "\n",
    "* Change the environment (modify the DeepRacer world's lighting conditions)\n",
    "* Automate the training process\n",
    "* Train on multiple tracks in parallel\n",
    "* Experiment! \n",
    "\n",
    "**Let's get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by installing libraries and helper functions which will be needed later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"common\")\n",
    "sys.path.append(\"./src\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from docker_utils import build_docker_image\n",
    "from docker_utils import push as docker_push\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "from IPython.display import Markdown\n",
    "from markdown_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get SageMaker execution role\n",
    "\n",
    "This notebook is designed to be run from a SageMaker notebook, so our first step is to get the SageMaker notebook's IAM role, so that we can start making calls against other AWS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sagemaker_role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    print('Unable to get role! Are you running this notebook locally?')\n",
    "\n",
    "print(\"Using Sagemaker IAM role arn: \\n{}\".format(sagemaker_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please note that this notebook cannot be run in `SageMaker local mode` as the simulator is based on AWS RoboMaker service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a copy of the DeepRacer simapp\n",
    "\n",
    "This is a docker container of the simulation application loaded into robomaker. A copy is downloaded and then some files are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the name of the simapp that is locally created and pushed to your account ECR\n",
    "local_simapp_ecr_docker_image_name = \"deepracer-sim-local-notebook\"\n",
    "public_ecr_alias = \"k1d3r4z1\"\n",
    "\n",
    "# Clean up existing docker images\n",
    "!if [ -n \"$(docker ps -a -q)\" ]; then docker rm -f $(docker ps -a -q); fi\n",
    "!if [ -n \"$(docker images -q)\" ]; then docker rmi -f $(docker images -q); fi\n",
    "\n",
    "!aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n",
    "\n",
    "!docker pull public.ecr.aws/{public_ecr_alias}/deepracer-sim-public\n",
    "\n",
    "!docker tag public.ecr.aws/{public_ecr_alias}/deepracer-sim-public {local_simapp_ecr_docker_image_name}\n",
    "\n",
    "# Get docker id and container id\n",
    "simapp_docker_ids = !docker images | grep deepracer-sim-public | tr -s ' '| cut -d ' ' -f 3 | head -n 1\n",
    "simapp_docker_id = simapp_docker_ids[0]\n",
    "simapp_container_ids = !docker run -d -t {simapp_docker_id}\n",
    "simapp_container_id = simapp_container_ids[0]\n",
    "\n",
    "# Copy all the required training related code and update the code base\n",
    "!docker cp {simapp_container_id}:/opt/amazon/markov ./src/\n",
    "!docker cp {simapp_container_id}:/opt/amazon/rl_coach.patch ./src/\n",
    "!docker cp {simapp_container_id}:/opt/ml/code/. ./src/lib/\n",
    "!rm ./src/lib/credentials #Bug in Jupyter can't handle symlinks to files w/o read perms\n",
    "    \n",
    "# Copy out the RoboMaker 3D environment\n",
    "!docker cp {simapp_container_id}:/opt/amazon/install/deepracer_simulation_environment ./src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the instance type\n",
    "instance_type = \"ml.c4.2xlarge\"\n",
    "# instance_type = \"ml.p2.xlarge\"\n",
    "# instance_type = \"ml.c5.4xlarge\"\n",
    "\n",
    "# Starting SageMaker session\n",
    "sage_session = sagemaker.session.Session()\n",
    "\n",
    "# Create unique job name.\n",
    "job_name_prefix = \"deepracer-notebook\"\n",
    "\n",
    "# Duration of job in seconds (1200 seconds = 20 minutes)\n",
    "job_duration_in_seconds = 1200\n",
    "\n",
    "# AWS RoboMaker (which we need for our simulation) is only supported in certain regions\n",
    "supported_regions = [\n",
    "    \"us-east-1\",\n",
    "    \"us-east-2\",\n",
    "    \"us-west-2\",\n",
    "    \"ap-southeast-1\",\n",
    "    \"ap-northeast-1\",\n",
    "    \"eu-central-1\",\n",
    "    \"eu-west-1\",\n",
    "    \"us-gov-west-1\"\n",
    "]\n",
    "\n",
    "# Check that we are in a region that supports RoboMaker\n",
    "aws_region = sage_session.boto_region_name\n",
    "if aws_region not in supported_regions:\n",
    "    raise Exception(\n",
    "        \"This notebook uses RoboMaker which is available only in certain\"\n",
    "        \"regions. Please switch to one of these regions.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Sagemaker docker image\n",
    "\n",
    "The file ./Dockerfile contains all the packages that should be included in the docker container image. Instead of using the default SageMaker container, we will be using this docker container. This is a separate docker container than the one created earlier for RoboMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from copy_to_sagemaker_container import (\n",
    "    get_sagemaker_docker,\n",
    "    copy_to_sagemaker_container,\n",
    "    get_custom_image_name,\n",
    ")\n",
    "\n",
    "cpu_or_gpu = \"gpu\" if instance_type.startswith(\"ml.p\") else \"cpu\"\n",
    "repository_short_name = \"sagemaker-docker-%s\" % cpu_or_gpu\n",
    "custom_image_name = get_custom_image_name(repository_short_name)\n",
    "try:\n",
    "    print(\"Copying files from your notebook to existing sagemaker container\")\n",
    "    sagemaker_docker_id = get_sagemaker_docker(repository_short_name)\n",
    "    copy_to_sagemaker_container(sagemaker_docker_id, repository_short_name)\n",
    "except Exception as e:\n",
    "    print(\"Creating sagemaker container\")\n",
    "    docker_build_args = {\n",
    "        \"CPU_OR_GPU\": cpu_or_gpu,\n",
    "        \"AWS_REGION\": boto3.Session().region_name,\n",
    "    }\n",
    "    build_docker_image(\n",
    "        repository_short_name, build_args=docker_build_args\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run these commands if you wish to modify the SageMaker and Robomaker code\n",
    "<span style=\"color:red\">Note: Make sure you have atleast 25 GB of space when you are planning to modify the Sagemaker and Robomaker code</span>\n",
    "\n",
    "Note that the code below will copy any changes you made to `deepracer_simulation_environment` back into the container created during the build, so these changes will end up in your docker container image. This allows you to customize the DeepRacer simulation environment. \n",
    "\n",
    "**Pro tip: if you are planning to make simple changes to the environment such as adding additional lights or changing the intensity or color of lights, you can do this in real time inside Gazebo, by launching the simulation job's Gazebo viewer from inside the AWS RoboMaker console!** This is typically faster and easier than changing the container image.\n",
    "\n",
    "#### Accessing Gazebo from RoboMaker\n",
    "\n",
    "Find the RoboMaker simulation job, and look for a \"gazebo\" tool under \"Simulation application tools\": \n",
    "\n",
    "![Gazebo Launcher](./gazebo_launcher.png)\n",
    "\n",
    "After clicking connect, a window should open where you can interact with the Gazebo environment. Try changing or adding some lights! \n",
    "\n",
    "![Gazebo Viewer](./gazebo_viewer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get docker id and container id\n",
    "simapp_docker_ids = !docker images | grep deepracer-sim-local-notebook | tr -s ' '| cut -d ' ' -f 3 | head -n 1\n",
    "simapp_docker_id = simapp_docker_ids[0]\n",
    "simapp_container_ids = !docker run -d -t {simapp_docker_id} /bin/sh\n",
    "simapp_container_id = simapp_container_ids[0]\n",
    "\n",
    "!docker cp ./src/markov {simapp_container_id}:/opt/amazon/\n",
    "!docker cp ./src/rl_coach.patch {simapp_container_id}:/opt/amazon/\n",
    "# #Restore symlink removed earlier due to bug in Jupyter\n",
    "!rm ./src/lib/credentials\n",
    "!ln -s /root/.aws/credentials ./src/lib/credentials\n",
    "!docker cp ./src/lib/. {simapp_container_id}:/opt/ml/code/.\n",
    "\n",
    "# #This is the Robomaker 3d environment\n",
    "!docker cp ./src/deepracer_simulation_environment {simapp_container_id}:/opt/amazon/install/\n",
    "\n",
    "# #Only needed if one modifies the ROS packages/libraries/etc\n",
    "#!docker exec {simapp_container_id} /opt/ml/code/scripts/build_deepracer_ros_packages.sh\n",
    "\n",
    "!docker exec {simapp_container_id} /opt/ml/code/scripts/clean_up_local.sh\n",
    "\n",
    "!docker stop {simapp_container_id}\n",
    "\n",
    "!docker commit {simapp_container_id} deepracer-sim-local-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload container images to ECR\n",
    "\n",
    "The Robot Application and Simulation Application containers need to be pushed to ECR before we can create a Simulation Job in RoboMaker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Push the simapp docker image to your ECR account \n",
    "docker_push(local_simapp_ecr_docker_image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the SageMaker docker image to your ECR account \n",
    "custom_image_name = docker_push(repository_short_name)\n",
    "print(\"Using ECR image %s\" % custom_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Already built the container images? Start here! \n",
    "\n",
    "Unless you need to change the simulation environment (which requires rebuilding the container images above), you should always restart your notebook from here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an S3 bucket\n",
    "\n",
    "We will need an S3 bucket to store model checkpoint data and logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket\n",
    "s3_bucket = sage_session.default_bucket()\n",
    "\n",
    "# SDK appends the job name and output folder\n",
    "s3_output_path = \"s3://{}/\".format(s3_bucket)\n",
    "\n",
    "# Ensure that the S3 prefix contains the keyword 'sagemaker'\n",
    "s3_prefix = job_name_prefix + \"-sagemaker-\" + strftime(\"%y%m%d-%H%M%S\", gmtime())\n",
    "\n",
    "# Get the AWS account id of this account\n",
    "sts = boto3.client(\"sts\")\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(\"Using s3 bucket {}\".format(s3_bucket))\n",
    "print(\n",
    "    \"Model checkpoints and other metadata will be stored at: \\ns3://{}/{}\".format(\n",
    "        s3_bucket, s3_prefix\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query VPC configuration\n",
    "\n",
    "We need our containers to be able to communicate over the network, so we need to determine our VPC network configuration.\n",
    "\n",
    "**Note: This code assumes that the *default* VPC is being used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.client(\"ec2\")\n",
    "\n",
    "print(\"Using the default VPC\")\n",
    "deepracer_vpc = [vpc[\"VpcId\"] for vpc in ec2.describe_vpcs()[\"Vpcs\"] if vpc[\"IsDefault\"] == True][0]\n",
    "\n",
    "deepracer_security_groups = [\n",
    "    group[\"GroupId\"]\n",
    "    for group in ec2.describe_security_groups()[\"SecurityGroups\"]\n",
    "    if \"VpcId\" in group and group[\"GroupName\"] == \"default\" and group[\"VpcId\"] == deepracer_vpc\n",
    "]\n",
    "\n",
    "deepracer_subnets = [\n",
    "    subnet[\"SubnetId\"]\n",
    "    for subnet in ec2.describe_subnets()[\"Subnets\"]\n",
    "    if subnet[\"VpcId\"] == deepracer_vpc and subnet[\"DefaultForAz\"] == True\n",
    "]\n",
    "\n",
    "print(\"Using VPC:\", deepracer_vpc)\n",
    "print(\"Using security group:\", deepracer_security_groups)\n",
    "print(\"Using subnets:\", deepracer_subnets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an S3 Endpoint inside the VPC\n",
    "\n",
    "We want to access S3 over the VPC to avoid network egress charges (and improve speed and security).\n",
    "\n",
    "The default VPC should already have an S3 endpoint, but just in case it does not, we create one here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vpc_endpoint_table():\n",
    "    print(\"Creating \")\n",
    "    try:\n",
    "        route_tables = [\n",
    "            route_table[\"RouteTableId\"]\n",
    "            for route_table in ec2.describe_route_tables()[\"RouteTables\"]\n",
    "            if route_table[\"VpcId\"] == deepracer_vpc\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        if \"UnauthorizedOperation\" in str(e):\n",
    "            display(Markdown(generate_help_for_s3_endpoint_permissions(sagemaker_role)))\n",
    "        else:\n",
    "            display(Markdown(create_s3_endpoint_manually(aws_region, deepracer_vpc)))\n",
    "        raise e\n",
    "\n",
    "    print(\"Trying to attach S3 endpoints to the following route tables:\", route_tables)\n",
    "\n",
    "    if not route_tables:\n",
    "        raise Exception(\n",
    "            (\n",
    "                \"No route tables were found. Please follow the VPC S3 endpoint creation \"\n",
    "                \"guide by clicking the above link.\"\n",
    "            )\n",
    "        )\n",
    "    try:\n",
    "        ec2.create_vpc_endpoint(\n",
    "            DryRun=False,\n",
    "            VpcEndpointType=\"Gateway\",\n",
    "            VpcId=deepracer_vpc,\n",
    "            ServiceName=\"com.amazonaws.{}.s3\".format(aws_region),\n",
    "            RouteTableIds=route_tables,\n",
    "        )\n",
    "        print(\"S3 endpoint created successfully!\")\n",
    "    except Exception as e:\n",
    "        if \"RouteAlreadyExists\" in str(e):\n",
    "            print(\"S3 endpoint already exists.\")\n",
    "        elif \"UnauthorizedOperation\" in str(e):\n",
    "            display(Markdown(generate_help_for_s3_endpoint_permissions(role)))\n",
    "            raise e\n",
    "        else:\n",
    "            display(Markdown(create_s3_endpoint_manually(aws_region, deepracer_vpc)))\n",
    "            raise e\n",
    "\n",
    "create_vpc_endpoint_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy configuration files to S3\n",
    "\n",
    "We need to copy our reward function and model settings to S3 so that our simulation job can see them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_location = \"s3://%s/%s\" % (s3_bucket, s3_prefix)\n",
    "print(s3_location)\n",
    "\n",
    "# Clean up the previously uploaded files\n",
    "!aws s3 rm --recursive {s3_location}\n",
    "\n",
    "!aws s3 cp ./src/artifacts/rewards/default.py {s3_location}/customer_reward_function.py\n",
    "\n",
    "!aws s3 cp ./src/artifacts/actions/default.json {s3_location}/model/model_metadata.json\n",
    "\n",
    "#!aws s3 cp src/markov/presets/default.py {s3_location}/presets/preset.py\n",
    "#!aws s3 cp src/markov/presets/preset_attention_layer.py {s3_location}/presets/preset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL model using the Python SDK Script mode\n",
    "\n",
    "Next, we define the following algorithm metrics that we want to capture from cloudwatch logs to monitor the training progress. These are algorithm specific parameters and might change for different algorithm. We use Clipped PPO by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    # Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=-102.88, Steps=19019, Training iteration=1\n",
    "    {\"Name\": \"reward-training\", \"Regex\": \"^Training>.*Total reward=(.*?),\"},\n",
    "    # Policy training> Surrogate loss=-0.32664725184440613, KL divergence=7.255815035023261e-06, Entropy=2.83156156539917, training epoch=0, learning_rate=0.00025\n",
    "    {\"Name\": \"ppo-surrogate-loss\", \"Regex\": \"^Policy training>.*Surrogate loss=(.*?),\"},\n",
    "    {\"Name\": \"ppo-entropy\", \"Regex\": \"^Policy training>.*Entropy=(.*?),\"},\n",
    "    # Testing> Name=main_level/agent, Worker=0, Episode=19, Total reward=1359.12, Steps=20015, Training iteration=2\n",
    "    {\"Name\": \"reward-testing\", \"Regex\": \"^Testing>.*Total reward=(.*?),\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_hyperparameter = {\n",
    "    \"s3_bucket\": s3_bucket,\n",
    "    \"s3_prefix\": s3_prefix,\n",
    "    \"aws_region\": aws_region,\n",
    "    \"model_metadata_s3_key\": \"%s/model/model_metadata.json\" % s3_prefix,\n",
    "    \"reward_function_s3_source\": \"%s/customer_reward_function.py\" % s3_prefix,\n",
    "    \"batch_size\": \"64\",\n",
    "    \"num_epochs\": \"10\",\n",
    "    \"stack_size\": \"1\",\n",
    "    \"lr\": \"0.0003\",\n",
    "    \"exploration_type\": \"Categorical\",\n",
    "    \"e_greedy_value\": \"1\",\n",
    "    \"epsilon_steps\": \"10000\",\n",
    "    \"beta_entropy\": \"0.01\",\n",
    "    \"discount_factor\": \"0.95\", # It's a good idea to reduce this from the default value of 0.999\n",
    "    \"loss_type\": \"Huber\",\n",
    "    \"num_episodes_between_training\": \"20\",\n",
    "    \"max_sample_count\": \"0\",\n",
    "    \"sampling_frequency\": \"1\"\n",
    "    #     ,\"pretrained_s3_bucket\": \"sagemaker-us-east-1-259455987231\"\n",
    "    #     ,\"pretrained_s3_prefix\": \"deepracer-notebook-sagemaker-200729-202318\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SageMaker so we can create our training job\n",
    "b_sagemaker = boto3.client(\"sagemaker\", region_name=aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if there is an existing, running training job (if there is, we do not need to create one)\n",
    "try:\n",
    "    job_arn = training_job['TrainingJobArn']\n",
    "except:\n",
    "    job_arn = 'none'\n",
    "\n",
    "if job_arn == 'none':\n",
    "    training_job = b_sagemaker.create_training_job(\n",
    "        TrainingJobName=s3_prefix,\n",
    "        HyperParameters=custom_hyperparameter,\n",
    "        AlgorithmSpecification={\n",
    "            \"TrainingImage\": \"{}:latest\".format(custom_image_name),\n",
    "            \"TrainingInputMode\": \"File\"\n",
    "        },\n",
    "        RoleArn=sagemaker_role,\n",
    "        OutputDataConfig={\n",
    "            \"S3OutputPath\": \"s3://{}/{}/train-output/\".format(s3_bucket, s3_prefix)\n",
    "        },\n",
    "        ResourceConfig={\n",
    "            'InstanceType': instance_type,\n",
    "            'InstanceCount': 1,\n",
    "            'VolumeSizeInGB': 32\n",
    "        },\n",
    "        VpcConfig={\n",
    "            'SecurityGroupIds': deepracer_security_groups,\n",
    "            'Subnets': deepracer_subnets\n",
    "        },\n",
    "        StoppingCondition={\n",
    "            'MaxRuntimeInSeconds': job_duration_in_seconds\n",
    "        },\n",
    "    )\n",
    "    \n",
    "job_name = s3_prefix\n",
    "training_job_arn = training_job['TrainingJobArn']\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Robomaker Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a new RoboMaker job and an associated Kinesis video stream, so we can watch the car go! \n",
    "robomaker = boto3.client(\"robomaker\")\n",
    "kinesisvideo = boto3.client(\"kinesisvideo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Simulation Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robomaker_environment = {\"uri\": get_custom_image_name(local_simapp_ecr_docker_image_name)+\":latest\"}\n",
    "simulation_software_suite = {\"name\": \"SimulationRuntime\"}\n",
    "robot_software_suite = {\"name\": \"General\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"deepracer-notebook-application\" + strftime(\"%y%m%d-%H%M%S\", gmtime())\n",
    "\n",
    "print(app_name)\n",
    "try:\n",
    "    response = robomaker.create_simulation_application(\n",
    "        name=app_name,\n",
    "        environment=robomaker_environment,\n",
    "        simulationSoftwareSuite=simulation_software_suite,\n",
    "        robotSoftwareSuite=robot_software_suite\n",
    "    )\n",
    "    simulation_app_arn = response[\"arn\"]\n",
    "    print(\"Created a new simulation app with ARN:\", simulation_app_arn)\n",
    "except Exception as e:\n",
    "    if \"AccessDeniedException\" in str(e):\n",
    "        display(Markdown(generate_help_for_robomaker_all_permissions(role)))\n",
    "        raise e\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the number of simulation jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this for multiple rollouts. This will invoke the specified number of robomaker jobs to collect experience\n",
    "num_simulation_workers = 1 # Let's try running 2 jobs at once! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Kinesis video stream(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kvs_stream_name=[]\n",
    "kvs_stream_arns=[]\n",
    "for job_no in range(num_simulation_workers):\n",
    "    kvs_stream_name.append(\"dr-kvs-{}-{}\".format(job_name,job_no))\n",
    "    try:\n",
    "        response=kinesisvideo.create_stream(StreamName=kvs_stream_name[job_no],MediaType=\"video/h264\",DataRetentionInHours=24)\n",
    "    except Exception as err:\n",
    "        if err.__class__.__name__ == 'ResourceInUseException':\n",
    "            response=kinesisvideo.describe_stream(StreamName=kvs_stream_name[job_no])[\"StreamInfo\"]\n",
    "        else:\n",
    "            raise err\n",
    "    print(\"Created kinesis video stream {}\".format(kvs_stream_name[job_no]))\n",
    "    kvs_stream_arns.append(response[\"StreamARN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Simulation job(s) on RoboMaker\n",
    "\n",
    "We create [AWS RoboMaker](https://console.aws.amazon.com/robomaker/home#welcome) Simulation Jobs that simulates the environment and shares this data with SageMaker for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_yaml_name = \"training_params.yaml\"\n",
    "world_name = \"2022_reinvent_champ\"\n",
    "\n",
    "with open(\"./src/artifacts/yaml/training_yaml_template.yaml\", \"r\") as filepointer:\n",
    "    yaml_config = yaml.safe_load(filepointer)\n",
    "\n",
    "yaml_config[\"WORLD_NAME\"] = world_name\n",
    "yaml_config[\"SAGEMAKER_SHARED_S3_BUCKET\"] = s3_bucket\n",
    "yaml_config[\"SAGEMAKER_SHARED_S3_PREFIX\"] = s3_prefix\n",
    "yaml_config[\"TRAINING_JOB_ARN\"] = training_job_arn\n",
    "yaml_config[\"METRICS_S3_BUCKET\"] = s3_bucket\n",
    "yaml_config[\"METRICS_S3_OBJECT_KEY\"] = \"{}/training_metrics.json\".format(s3_prefix)\n",
    "yaml_config[\"SIMTRACE_S3_BUCKET\"] = s3_bucket\n",
    "yaml_config[\"SIMTRACE_S3_PREFIX\"] = \"{}/iteration-data/training\".format(s3_prefix)\n",
    "yaml_config[\"AWS_REGION\"] = aws_region\n",
    "yaml_config[\"ROBOMAKER_SIMULATION_JOB_ACCOUNT_ID\"] = account_id\n",
    "yaml_config[\"KINESIS_VIDEO_STREAM_NAME\"] = kvs_stream_name[job_no]\n",
    "yaml_config[\"REWARD_FILE_S3_KEY\"] = \"{}/customer_reward_function.py\".format(s3_prefix)\n",
    "yaml_config[\"MODEL_METADATA_FILE_S3_KEY\"] = \"{}/model/model_metadata.json\".format(s3_prefix)\n",
    "yaml_config[\"NUM_WORKERS\"] = num_simulation_workers\n",
    "yaml_config[\"MP4_S3_BUCKET\"] = s3_bucket\n",
    "yaml_config[\"MP4_S3_OBJECT_PREFIX\"] = \"{}/iteration-data/training\".format(s3_prefix)\n",
    "\n",
    "# Race-type supported for training are TIME_TRIAL, OBJECT_AVOIDANCE, HEAD_TO_BOT\n",
    "# If you need to modify more attributes look at the template yaml file\n",
    "race_type = \"TIME_TRIAL\"\n",
    "\n",
    "if race_type == \"OBJECT_AVOIDANCE\":\n",
    "    yaml_config[\"NUMBER_OF_OBSTACLES\"] = \"6\"\n",
    "    yaml_config[\"RACE_TYPE\"] = \"OBJECT_AVOIDANCE\"\n",
    "\n",
    "elif race_type == \"HEAD_TO_BOT\":\n",
    "    yaml_config[\"NUMBER_OF_BOT_CARS\"] = \"6\"\n",
    "    yaml_config[\"RACE_TYPE\"] = \"HEAD_TO_BOT\"\n",
    "\n",
    "# Printing the modified yaml parameter\n",
    "for key, value in yaml_config.items():\n",
    "    print(\"{}: {}\".format(key.ljust(40, \" \"), value))\n",
    "\n",
    "# Uploading the modified yaml parameter\n",
    "with open(\"./training_params.yaml\", \"w\") as filepointer:\n",
    "    yaml.dump(yaml_config, filepointer)\n",
    "\n",
    "!aws s3 cp ./training_params.yaml {s3_location}/training_params.yaml\n",
    "!rm training_params.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = list()\n",
    "for job_no in range(num_simulation_workers):\n",
    "    response = robomaker.create_simulation_job(\n",
    "        clientRequestToken=strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()),\n",
    "        outputLocation={\n",
    "            \"s3Bucket\": s3_bucket,\n",
    "            \"s3Prefix\": s3_prefix\n",
    "        },\n",
    "        maxJobDurationInSeconds=job_duration_in_seconds,\n",
    "        iamRole=sagemaker_role,\n",
    "        failureBehavior=\"Fail\",\n",
    "        simulationApplications=[{\n",
    "            \"application\": simulation_app_arn,\n",
    "            \"applicationVersion\": \"$LATEST\",\n",
    "            \"launchConfig\": {\n",
    "                \"command\": [\"roslaunch\", \"deepracer_simulation_environment\", \"distributed_training.launch\"],\n",
    "                \"environmentVariables\": {\n",
    "                    \"S3_YAML_NAME\": s3_yaml_name,\n",
    "                    \"SAGEMAKER_SHARED_S3_PREFIX\": s3_prefix,\n",
    "                    \"SAGEMAKER_SHARED_S3_BUCKET\": s3_bucket,\n",
    "                    \"WORLD_NAME\": world_name,\n",
    "                    \"KINESIS_VIDEO_STREAM_NAME\": kvs_stream_name[job_no],\n",
    "                    \"APP_REGION\": aws_region,\n",
    "                    \"MODEL_METADATA_FILE_S3_KEY\": \"%s/model/model_metadata.json\" % s3_prefix,\n",
    "                    \"ROLLOUT_IDX\": str(job_no),\n",
    "                    \"DEEPRACER_JOB_TYPE_ENV\": \"SAGEONLY\"\n",
    "                },\n",
    "                \"streamUI\": True\n",
    "            },\n",
    "            \"uploadConfigurations\": [{\n",
    "                    \"name\": \"gazebo-logs\",\n",
    "                    \"path\": \"/root/.gazebo/server*/*.log\",\n",
    "                    \"uploadBehavior\": \"UPLOAD_ON_TERMINATE\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"ros-logs\",\n",
    "                    \"path\": \"/root/.ros/log/**\",\n",
    "                    \"uploadBehavior\": \"UPLOAD_ON_TERMINATE\"\n",
    "                }\n",
    "            ],\n",
    "            \"useDefaultUploadConfigurations\": False,\n",
    "            \"tools\": [{\n",
    "                \"streamUI\": True,\n",
    "                \"name\": \"rviz\",\n",
    "                \"command\": \"source /opt/ros/melodic/setup.bash;source /opt/amazon/install/setup.bash; rviz\",\n",
    "                \"streamOutputToCloudWatch\": True,\n",
    "                \"exitBehavior\": \"RESTART\"\n",
    "              },\n",
    "              {\n",
    "                \"streamUI\": True,\n",
    "                \"name\": \"terminal\",\n",
    "                \"command\": \"source /opt/ros/melodic/setup.bash;source /opt/amazon/install/setup.bash; xfce4-terminal\",\n",
    "                \"streamOutputToCloudWatch\": True,\n",
    "                \"exitBehavior\": \"RESTART\"\n",
    "              },\n",
    "              {\n",
    "                \"streamUI\": True,\n",
    "                \"name\": \"gazebo\",\n",
    "                \"command\": \"source /opt/ml/code/scripts/gzclient_source.sh; export GAZEBO_MODEL_PATH=/opt/amazon/install/deepracer_simulation_environment/share/deepracer_simulation_environment/; gzclient\",\n",
    "                \"streamOutputToCloudWatch\": True,\n",
    "                \"exitBehavior\": \"RESTART\"\n",
    "            }],\n",
    "        }],\n",
    "        vpcConfig={\n",
    "            \"subnets\": deepracer_subnets,\n",
    "            \"securityGroups\": deepracer_security_groups,\n",
    "            \"assignPublicIp\": True\n",
    "        }\n",
    "    )\n",
    "    responses.append(response)\n",
    "    time.sleep(5)\n",
    "    \n",
    "print(\"Created the following jobs:\")\n",
    "job_arns = [response[\"arn\"] for response in responses]\n",
    "for job_arn in job_arns:\n",
    "    print(\"Job ARN\", job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the simulations in RoboMaker\n",
    "You can visit the RoboMaker console to visualize the simulations or run the following cell to generate the hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(generate_robomaker_links(job_arns, aws_region)))\n",
    "\n",
    "for job_no in range(num_simulation_workers):\n",
    "    display(Markdown(\"View the Kinesis video stream <a target=_blank href=\\\"https://us-east-1.console.aws.amazon.com/kinesisvideo/home?region=us-east-1#/streams/streamName/%s\\\">here.</a> (Expand 'Media Playback')\"%(kvs_stream_name[job_no])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a folder to hold training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "print(\"Create local folder {}\".format(tmp_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the output\n",
    "\n",
    "Take a look at the metrics from the training job and examine the raw output logs and videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot metrics for training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "training_metrics_file = \"training_metrics.json\"\n",
    "training_metrics_path = \"{}/{}\".format(s3_prefix, training_metrics_file)\n",
    "wait_for_s3_object(s3_bucket, training_metrics_path, tmp_dir)\n",
    "\n",
    "json_file = \"{}/{}\".format(tmp_dir, training_metrics_file)\n",
    "with open(json_file) as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "df = pd.DataFrame(data[\"metrics\"])\n",
    "x_axis = \"episode\"\n",
    "y_axis = \"reward_score\"\n",
    "\n",
    "plt = df.plot(x=x_axis, y=y_axis, figsize=(12, 5), legend=True, style=\"b-\")\n",
    "plt.set_ylabel(y_axis)\n",
    "plt.set_xlabel(x_axis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore output logs and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"Visit <a target=_blank href=\\\"https://s3.console.aws.amazon.com/s3/buckets/%s?region=%s&prefix=%s/&showversions=false\\\">the output logs, videos, and other artifacts in S3.</a>\"%(s3_bucket,aws_region,s3_prefix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Your Model into the DeepRacer console\n",
    "\n",
    "When training is complete, import the trained model into the DeepRacer console so one can clone and train it further in the console, evaluate it in the console, or submit it to the virtual league. Visit <a href=\"https://us-east-1.console.aws.amazon.com/deepracer/home?region=us-east-1#models\">\"Your Models\"</a> in the DeepRacer console, click the 'Import model' button, and follow the directions. Use the following URL for your import path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"Copy and paste this S3 path: <a target=_blank href=\\\"s3://%s/%s\\\">s3://%s/%s</a>\"%(s3_bucket,s3_prefix,s3_bucket,s3_prefix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore output logs and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"Visit <a target=_blank href=\\\"https://s3.console.aws.amazon.com/s3/buckets/%s?region=%s&prefix=%s/&showversions=false\\\">the output logs, videos, and other artifacts in S3.</a>\"%(s3_bucket,aws_region,s3_prefix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your model\n",
    "\n",
    "Start an evaluation job to see how your model performs! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Time Trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3_yaml_name = \"evaluation_params.yaml\"\n",
    "world_name = \"2022_reinvent_champ\"\n",
    "\n",
    "with open(\"./src/artifacts/yaml/evaluation_yaml_template.yaml\", \"r\") as filepointer:\n",
    "    yaml_config = yaml.safe_load(filepointer)\n",
    "\n",
    "yaml_config[\"WORLD_NAME\"] = world_name\n",
    "yaml_config[\"MODEL_S3_BUCKET\"] = s3_bucket\n",
    "yaml_config[\"MODEL_S3_PREFIX\"] = s3_prefix\n",
    "yaml_config[\"AWS_REGION\"] = aws_region\n",
    "yaml_config[\"METRICS_S3_BUCKET\"] = s3_bucket\n",
    "yaml_config[\"METRICS_S3_OBJECT_KEY\"] = \"{}/evaluation_metrics.json\".format(s3_prefix)\n",
    "yaml_config[\"SIMTRACE_S3_BUCKET\"] = s3_bucket\n",
    "yaml_config[\"SIMTRACE_S3_PREFIX\"] = \"{}/iteration-data/evaluation\".format(s3_prefix)\n",
    "yaml_config[\"ROBOMAKER_SIMULATION_JOB_ACCOUNT_ID\"] = account_id\n",
    "yaml_config[\"NUMBER_OF_TRIALS\"] = \"5\"\n",
    "yaml_config[\"MP4_S3_BUCKET\"] = s3_bucket\n",
    "yaml_config[\"MP4_S3_OBJECT_PREFIX\"] = \"{}/iteration-data/evaluation\".format(s3_prefix)\n",
    "\n",
    "# Race-type supported for training are TIME_TRIAL, OBJECT_AVOIDANCE, HEAD_TO_BOT\n",
    "# If you need to modify more attributes look at the template yaml file\n",
    "race_type = \"TIME_TRIAL\"\n",
    "\n",
    "if race_type == \"OBJECT_AVOIDANCE\":\n",
    "    yaml_config[\"NUMBER_OF_OBSTACLES\"] = \"6\"\n",
    "    yaml_config[\"RACE_TYPE\"] = \"OBJECT_AVOIDANCE\"\n",
    "\n",
    "elif race_type == \"HEAD_TO_BOT\":\n",
    "    yaml_config[\"NUMBER_OF_BOT_CARS\"] = \"6\"\n",
    "    yaml_config[\"RACE_TYPE\"] = \"HEAD_TO_BOT\"\n",
    "\n",
    "# Printing the modified yaml parameter\n",
    "for key, value in yaml_config.items():\n",
    "    print(\"{}: {}\".format(key.ljust(40, \" \"), value))\n",
    "\n",
    "# Uploading the modified yaml parameter\n",
    "with open(\"./evaluation_params.yaml\", \"w\") as filepointer:\n",
    "    yaml.dump(yaml_config, filepointer)\n",
    "\n",
    "!aws s3 cp ./evaluation_params.yaml {s3_location}/evaluation_params.yaml\n",
    "!rm evaluation_params.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Kinesis video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of simultaneous evaluations to carry out\n",
    "num_evaluation_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvs_stream_name=[]\n",
    "kvs_stream_arns=[]\n",
    "for job_no in range(num_evaluation_workers):\n",
    "    kvs_stream_name.append(\"dr-kvs-{}-{}\".format(job_name,job_no))\n",
    "    try:\n",
    "        response=kinesisvideo.create_stream(StreamName=kvs_stream_name[job_no],MediaType=\"video/h264\",DataRetentionInHours=24)\n",
    "    except Exception as err:\n",
    "        if err.__class__.__name__ == 'ResourceInUseException':\n",
    "            response=kinesisvideo.describe_stream(StreamName=kvs_stream_name[job_no])[\"StreamInfo\"]\n",
    "        else:\n",
    "            raise err\n",
    "    print(\"Created kinesis video stream {}\".format(kvs_stream_name[job_no]))\n",
    "    kvs_stream_arns.append(response[\"StreamARN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses = list()\n",
    "for job_no in range(num_evaluation_workers):\n",
    "    response = robomaker.create_simulation_job(\n",
    "        clientRequestToken=strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()),\n",
    "        outputLocation={\n",
    "            \"s3Bucket\": s3_bucket,\n",
    "            \"s3Prefix\": s3_prefix\n",
    "        },\n",
    "        maxJobDurationInSeconds=job_duration_in_seconds,\n",
    "        iamRole=sagemaker_role,\n",
    "        failureBehavior=\"Fail\",\n",
    "        simulationApplications=[{\n",
    "            \"application\": simulation_app_arn,\n",
    "            \"applicationVersion\": \"$LATEST\",\n",
    "            \"launchConfig\": {\n",
    "                \"command\": [\"roslaunch\", \"deepracer_simulation_environment\", \"evaluation.launch\"],\n",
    "                \"environmentVariables\": {\n",
    "                    \"S3_YAML_NAME\": s3_yaml_name,\n",
    "                    \"MODEL_S3_PREFIX\": s3_prefix,\n",
    "                    \"MODEL_S3_BUCKET\": s3_bucket,\n",
    "                    \"WORLD_NAME\": world_name,\n",
    "                    \"KINESIS_VIDEO_STREAM_NAME\": kvs_stream_name[job_no],\n",
    "                    \"APP_REGION\": aws_region,\n",
    "                    \"MODEL_METADATA_FILE_S3_KEY\": \"%s/model/model_metadata.json\" % s3_prefix,\n",
    "                },\n",
    "                \"streamUI\": True\n",
    "            },\n",
    "            \"uploadConfigurations\": [{\n",
    "                    \"name\": \"gazebo-logs\",\n",
    "                    \"path\": \"/root/.gazebo/server*/*.log\",\n",
    "                    \"uploadBehavior\": \"UPLOAD_ON_TERMINATE\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"ros-logs\",\n",
    "                    \"path\": \"/root/.ros/log/**\",\n",
    "                    \"uploadBehavior\": \"UPLOAD_ON_TERMINATE\"\n",
    "                }\n",
    "            ],\n",
    "            \"useDefaultUploadConfigurations\": False,\n",
    "            \"tools\": [{\n",
    "                \"streamUI\": True,\n",
    "                \"name\": \"rviz\",\n",
    "                \"command\": \"source /opt/ros/melodic/setup.bash;source /opt/amazon/install/setup.bash; rviz\",\n",
    "                \"streamOutputToCloudWatch\": True,\n",
    "                \"exitBehavior\": \"RESTART\"\n",
    "              },\n",
    "              {\n",
    "                \"streamUI\": True,\n",
    "                \"name\": \"terminal\",\n",
    "                \"command\": \"source /opt/ros/melodic/setup.bash;source /opt/amazon/install/setup.bash; xfce4-terminal\",\n",
    "                \"streamOutputToCloudWatch\": True,\n",
    "                \"exitBehavior\": \"RESTART\"\n",
    "              },\n",
    "              {\n",
    "                \"streamUI\": True,\n",
    "                \"name\": \"gazebo\",\n",
    "                \"command\": \"source /opt/ml/code/scripts/gzclient_source.sh; export GAZEBO_MODEL_PATH=/opt/amazon/install/deepracer_simulation_environment/share/deepracer_simulation_environment/; gzclient\",\n",
    "                \"streamOutputToCloudWatch\": True,\n",
    "                \"exitBehavior\": \"RESTART\"\n",
    "            }],\n",
    "        }],\n",
    "        vpcConfig={\n",
    "            \"subnets\": deepracer_subnets,\n",
    "            \"securityGroups\": deepracer_security_groups,\n",
    "            \"assignPublicIp\": True\n",
    "        }\n",
    "    )\n",
    "    responses.append(response)\n",
    "    time.sleep(5)\n",
    "    \n",
    "print(\"Created the following jobs:\")\n",
    "job_arns = [response[\"arn\"] for response in responses]\n",
    "for job_arn in job_arns:\n",
    "    print(\"Job ARN\", job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the simulations in RoboMaker\n",
    "\n",
    "You can visit the RoboMaker console directly to watch the simulations, or run the cell below to generate Kinesis video stream URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(generate_robomaker_links(job_arns, aws_region)))\n",
    "\n",
    "for job_no in range(num_evaluation_workers):\n",
    "    display(Markdown(\"View the Kinesis video stream <a target=_blank href=\\\"https://us-east-1.console.aws.amazon.com/kinesisvideo/home?region=us-east-1#/streams/streamName/%s\\\">here.</a> (Expand 'Media Playback')\"%(kvs_stream_name[job_no])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create (another) temporary folder to plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_file = \"evaluation_metrics.json\"\n",
    "evaluation_metrics_path = \"{}/{}\".format(s3_prefix, evaluation_metrics_file)\n",
    "wait_for_s3_object(s3_bucket, evaluation_metrics_path, tmp_dir)\n",
    "\n",
    "json_file = \"{}/{}\".format(tmp_dir, evaluation_metrics_file)\n",
    "with open(json_file) as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "df = pd.DataFrame(data[\"metrics\"])\n",
    "# Converting milliseconds to seconds\n",
    "df[\"elapsed_time\"] = df[\"elapsed_time_in_milliseconds\"] / 1000\n",
    "df = df[[\"trial\", \"completion_percentage\", \"reset_count\", \"elapsed_time\"]]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore output logs and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"Visit <a target=_blank href=\\\"https://s3.console.aws.amazon.com/s3/buckets/%s?region=%s&prefix=%s/&showversions=false\\\">the output logs, videos, and other artifacts in S3.</a>\"%(s3_bucket,aws_region,s3_prefix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up RoboMaker and SageMaker training jobs\n",
    "\n",
    "Get rid of any outstanding RoboMaker and SageMaker training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancelling robomaker job\n",
    "for job_arn in job_arns:\n",
    "    try:\n",
    "        robomaker.cancel_simulation_job(job=job_arn)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Stopping sagemaker training job\n",
    "try:\n",
    "    sage_session.sagemaker_client.stop_training_job(TrainingJobName=job_name)\n",
    "except Exception as err:\n",
    "    print(\"Could not stop training job; already stopped?\",err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up Simulation Application Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robomaker.delete_simulation_application(application=simulation_app_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Kinesis Video Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for streamarn in kvs_stream_arns:\n",
    "    try:\n",
    "        kinesisvideo.delete_stream(StreamARN=streamarn)\n",
    "        print(\"Deleted\",streamarn)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean your S3 bucket\n",
    "\n",
    "**Note** this section is left commented to avoid accidentally deleting trained models you might want to keep! Please import your models into DeepRacer or move them to another S3 bucket, if you want to keep them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you only want to clean the s3 bucket\n",
    "sagemaker_s3_folder = \"s3://{}/{}\".format(s3_bucket, s3_prefix)\n",
    "!aws s3 rm --recursive {sagemaker_s3_folder}\n",
    "\n",
    "robomaker_s3_folder = \"s3://{}/{}\".format(s3_bucket, job_name)\n",
    "!aws s3 rm --recursive {robomaker_s3_folder}\n",
    "\n",
    "robomaker_sim_app = \"s3://{}/{}\".format(s3_bucket, 'robomaker')\n",
    "!aws s3 rm --recursive {robomaker_sim_app}\n",
    "\n",
    "model_output = \"s3://{}/{}\".format(s3_bucket, s3_bucket)\n",
    "!aws s3 rm --recursive {model_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the docker images from Elastic Container Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecr = boto3.client('ecr')\n",
    "\n",
    "ecr.delete_repository(repositoryName=local_simapp_ecr_docker_image_name,force=True)\n",
    "ecr.delete_repository(repositoryName=repository_short_name,force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the docker images\n",
    "Uncomment and run this only when you want to completely remove the docker containers or clean up the space of the sagemaker instance on this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker rm -f $(docker ps -a -q);\n",
    "!docker rmi -f $(docker images -q);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
